{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f314056-f1db-49a0-b4d4-47d6dcda71cb",
   "metadata": {},
   "source": [
    "# FrictionShift\n",
    "\n",
    "#### Usage:\n",
    "- Place your file (e.g. biomass_sample.csv or an .icsv with a metadata block) in the same folder.\n",
    "    \n",
    "- Run: python validate_icsv.py\n",
    "\n",
    "#### General Schematic:\n",
    "- icsv -> metadata + data -> frictionless schema -> metadata checks -> data validation -> human report\n",
    "\n",
    "#### Detailed Description:\n",
    "- Ingests an iCSV — split_metadata_and_csv() reads the file and heuristically separates metadata-like lines at the top from the CSV table. The main entry validate_icsv_file() accepts any file path (CSV or iCSV-like).\n",
    "  \n",
    "- Recognizes DATA vs METADATA — the heuristic looks for key: value metadata lines and for a CSV/header start; metadata is returned as a dict. This version assumes iCSV files always contain explicit \"METADATA:\" and \"Data:\" sections, and uses those markers to split metadata from tabular data. If the markers are not found, it falls back to heuristics.\n",
    "  \n",
    "- Uses METADATA to create frictionless SCHEMA — build_frictionless_schema_from_metadata() will parse fields or a JSON schema in metadata if present. If metadata is absent or insufficient, it falls back to a provided descriptor (your example) or infers a minimal schema from the DataFrame.\n",
    "  \n",
    "- Checks metadata structural guidelines & completeness — check_metadata_completeness() validates required keys (configurable list) and returns human-friendly problems. Validates data against the schema with frictionless & reports — uses frictionless.validate() and collects errors; human_readable_report() assembles clear suggestions for fixes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02a2a00-ea18-498c-a1ce-01e12d3ba106",
   "metadata": {},
   "source": [
    "### Writing an iCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65a5c043-9ffb-493b-a875-409cbd496d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowpat import pysmet as smet\n",
    "from snowpat import snowpackreader as spr\n",
    "from snowpat import icsv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f1ad7a4-4b88-4296-bd7c-15d866d257e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    [15.3, 60, 1013],  # T (Temperature), RH (Relative Humidity), P (Pressure)\n",
    "    [14.8, 62, 1012],\n",
    "    [16.1, 59, 1014]\n",
    "]\n",
    "\n",
    "# create a file object\n",
    "file = icsv.iCSVFile()\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "data_pd = pd.DataFrame(data, columns=[\"T\", \"RH\", \"P\"])\n",
    "file.setData(data_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65f7a7f0-6b68-44ac-9379-5121f97e078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set minimal metadata\n",
    "file.metadata.set_attribute(\"field_delimiter\", \":\")\n",
    "file.metadata.set_attribute(\"geometry\", \"POINT(7.1 46.2)\")\n",
    "file.metadata.set_attribute(\"srid\", \"EPSG:4326\")\n",
    "\n",
    "# Define output filename before writing\n",
    "out_filename = \"output_modified.icsv\"\n",
    "file.write(out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "589d8cba-0667-47a3-85a5-c596d5d45adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: output_modified.icsv\n",
      "METADATA:\n",
      "Required:\n",
      "field_delimiter : :\n",
      "geometry : POINT(7.1 46.2)\n",
      "srid : EPSG:4326\n",
      "Recommended:\n",
      "\n",
      "ACDD Attributes:\n",
      "Unknown Attributes:\n",
      "field_delimiter: :\n",
      "geometry: POINT(7.1 46.2)\n",
      "srid: EPSG:4326\n",
      "\n",
      "Other Metadata:\n",
      "\n",
      "Fields: ['T', 'RH', 'P']\n",
      "Recommended Fields:\n",
      "\n",
      "Other Fields:\n",
      "\n",
      "Geometry: None\n",
      "SRID: None\n",
      "Location: X: None\n",
      "Y: None\n",
      "Z: None\n",
      "EPSG: None\n",
      "\n",
      "Data:\n",
      "      T  RH     P\n",
      "0  15.3  60  1013\n",
      "1  14.8  62  1012\n",
      "2  16.1  59  1014\n",
      "METADATA:\n",
      "Required:\n",
      "field_delimiter : :\n",
      "geometry : POINT(7.1 46.2)\n",
      "srid : EPSG:4326\n",
      "Recommended:\n",
      "\n",
      "ACDD Attributes:\n",
      "Unknown Attributes:\n",
      "field_delimiter: :\n",
      "geometry: POINT(7.1 46.2)\n",
      "srid: EPSG:4326\n",
      "\n",
      "Other Metadata:\n",
      "\n",
      "Fields: ['T', 'RH', 'P']\n",
      "Recommended Fields:\n",
      "\n",
      "Other Fields:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#reading an iCSV\n",
    "filename = \"/Users/nunezcha/Documents/envidat_frictionless/output.icsv\"\n",
    "File = icsv.read(filename)\n",
    "data_pandas = file.data\n",
    "data_xarray = file.to_xarray()\n",
    "# metadata and fields can be accessed with get_attribute\n",
    "field_delimiter = file.metadata.get_attribute(\"field_delimiter\")\n",
    "fields = file.fields.get_attribute(\"fields\")\n",
    "# required keys will always be present, as a sanity check is done. Any other might return None if it is not available.\n",
    "# To see what metadata is available, you can print the information:\n",
    "file.info() # prints information about the whole file\n",
    "print(file.metadata) # prints information on the metadata only\n",
    "print(file.fields) # print information on the fields section\n",
    "\n",
    "# changing metadata\n",
    "file.metadata.set_attribute(\"field_delimiter\", \":\")\n",
    "\n",
    "# and for writing to an output again (if no output filename is provided, the given filename is used with an out flag):\n",
    "file.write(out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a1188a-845d-44dd-896f-b1a325a044a5",
   "metadata": {},
   "source": [
    "### Decoupling Metadata and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99c58764-2306-4f93-a3b1-24f3544728a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Any, List, Optional\n",
    "import pandas as pd\n",
    "from frictionless import Schema, Resource, validate\n",
    "from io import StringIO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a2a8b19-f72a-4e2a-b676-d221bdfbac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Configuration: expected metadata keys & fallback schema ----------\n",
    "REQUIRED_METADATA_KEYS = [\n",
    "    \"field_delimiter\",  # example key often present in icsv\n",
    "    # Add any other keys you want to require\n",
    "]\n",
    "\n",
    "FALLBACK_METADATA_SCHEMA_DESCRIPTOR = {\n",
    "    \"fields\": [\n",
    "        {\"name\": \"Site.ID\", \"type\": \"integer\", \"constraints\": {\"required\": True}},\n",
    "        {\"name\": \"Biomasstype\", \"type\": \"string\", \"constraints\": {\"required\": True, \"enum\": [\"Living\", \"Litter\"]}},\n",
    "        {\"name\": \"Site\", \"type\": \"string\", \"constraints\": {\"required\": True}},\n",
    "        {\"name\": \"Invasion\", \"type\": \"string\", \"constraints\": {\"required\": True, \"enum\": [\"Native\", \"Invaded\"]}},\n",
    "        {\"name\": \"Treatment\", \"type\": \"string\", \"constraints\": {\"required\": True, \"enum\": [\"Open\", \"No livestock\", \"No mammals\", \"No insects\"]}},\n",
    "        {\"name\": \"Weight_20by100_cm\", \"type\": \"number\", \"constraints\": {\"required\": True, \"minimum\": 0}},\n",
    "        {\"name\": \"sample_type\", \"type\": \"string\", \"constraints\": {\"required\": False}},\n",
    "    ],\n",
    "    \"missingValues\": [\"\", \"NA\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5393e40b-5658-428c-bcec-50cac489e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Helpers: parse file into metadata dict + pandas DataFrame ----------\n",
    "def split_metadata_and_csv(file_path: Path, sniff_lines: int = 500) -> Tuple[Dict[str, str], str]:\n",
    "    \"\"\"\n",
    "    Split an iCSV file into metadata dict and CSV text.\n",
    "    Behavior:\n",
    "      - If the file contains a \"METADATA:\" marker (case-insensitive), collect everything\n",
    "        from after that marker up to the \"Data:\" marker as metadata.\n",
    "      - Parse key:value lines from that block into metadata dict.\n",
    "      - Find the first likely CSV header line after \"Data:\" and return the CSV text from there.\n",
    "      - If markers are not present, fall back to heuristics (previous implementation).\n",
    "    \"\"\"\n",
    "    text = file_path.read_text(encoding=\"utf-8\")\n",
    "    lines = text.splitlines()\n",
    "    n = len(lines)\n",
    "\n",
    "    # Search for METADATA: and Data: markers (case-insensitive)\n",
    "    meta_idx = None\n",
    "    data_idx = None\n",
    "    for i, line in enumerate(lines[:sniff_lines]):\n",
    "        if re.match(r'^\\s*METADATA\\s*:?\\s*$', line, flags=re.IGNORECASE):\n",
    "            meta_idx = i\n",
    "            # find data marker after metadata\n",
    "            for j in range(i + 1, min(n, sniff_lines)):\n",
    "                if re.match(r'^\\s*Data\\s*:?\\s*$', lines[j], flags=re.IGNORECASE):\n",
    "                    data_idx = j\n",
    "                    break\n",
    "            break\n",
    "        # also handle the case \"METADATA: key: value\" where metadata content starts on same line\n",
    "        m_inline = re.match(r'^\\s*METADATA\\s*:\\s*(.*)$', line, flags=re.IGNORECASE)\n",
    "        if m_inline:\n",
    "            meta_idx = i\n",
    "            # create a pseudo next line with the inline content\n",
    "            # but prefer to keep parsing below; find Data: anyway\n",
    "            for j in range(i + 1, min(n, sniff_lines)):\n",
    "                if re.match(r'^\\s*Data\\s*:?\\s*$', lines[j], flags=re.IGNORECASE):\n",
    "                    data_idx = j\n",
    "                    break\n",
    "            break\n",
    "\n",
    "    # If we found both markers, use them deterministically\n",
    "    metadata_lines = []\n",
    "    csv_start_index = 0\n",
    "    if meta_idx is not None:\n",
    "        # Determine metadata block: lines between meta_idx and data_idx (if present)\n",
    "        start = meta_idx + 1\n",
    "        end = data_idx if data_idx is not None else min(n, sniff_lines)\n",
    "        # collect block\n",
    "        metadata_lines = lines[start:end]\n",
    "        # If there was inline content on the METADATA: line, try extracting it\n",
    "        inline_meta = re.match(r'^\\s*METADATA\\s*:\\s*(.*)$', lines[meta_idx], flags=re.IGNORECASE)\n",
    "        if inline_meta and inline_meta.group(1).strip():\n",
    "            metadata_lines.insert(0, inline_meta.group(1).strip())\n",
    "\n",
    "        # Determine where the CSV starts: after the Data: marker if present\n",
    "        if data_idx is not None:\n",
    "            # find the first non-empty line after data_idx that looks like a header (contains a delimiter)\n",
    "            for k in range(data_idx + 1, n):\n",
    "                l = lines[k].strip()\n",
    "                if not l:\n",
    "                    continue\n",
    "                # treat as header if it contains comma, semicolon, tab, or many words separated by spaces (fallback)\n",
    "                if any(d in l for d in [\",\", \";\", \"\\t\", \"|\", \":\"]):\n",
    "                    csv_start_index = k\n",
    "                    break\n",
    "                # If line looks like CSV header without delimiters (rare), accept if next line has numeric tokens\n",
    "                next_line = lines[k + 1].strip() if k + 1 < n else \"\"\n",
    "                # quick heuristic: header with words and next has digits or 'NA' tokens\n",
    "                if re.search(r'[A-Za-z]', l) and re.search(r'(\\d|NA)', next_line):\n",
    "                    csv_start_index = k\n",
    "                    break\n",
    "            else:\n",
    "                # no clear header found, set csv_start_index to data_idx + 1 (will be handled later)\n",
    "                csv_start_index = data_idx + 1\n",
    "        else:\n",
    "            # no explicit Data: marker; fallback to first line after metadata block\n",
    "            csv_start_index = end\n",
    "    else:\n",
    "        # fallback: previous heuristics (scan for leading key:value pairs or detect header)\n",
    "        # collect leading \"key: value\" style lines as metadata candidates\n",
    "        for i, line in enumerate(lines[:sniff_lines]):\n",
    "            if re.search(r':', line) and line.count(',') <= 1:\n",
    "                metadata_lines.append(line)\n",
    "                continue\n",
    "            if ',' in line or ';' in line or '\\t' in line:\n",
    "                delim = ',' if ',' in line else (';' if ';' in line else '\\t')\n",
    "                header_count = line.count(delim)\n",
    "                similar = 0\n",
    "                for nxt in lines[i+1:i+6]:\n",
    "                    if nxt.count(delim) >= header_count:\n",
    "                        similar += 1\n",
    "                if similar >= 1:\n",
    "                    csv_start_index = i\n",
    "                    break\n",
    "        else:\n",
    "            csv_start_index = 0\n",
    "\n",
    "    # Build metadata dict from metadata_lines: parse key: value entries\n",
    "    metadata = {}\n",
    "    for ml in metadata_lines:\n",
    "        ml_strip = ml.strip()\n",
    "        if not ml_strip:\n",
    "            continue\n",
    "        # some metadata blocks include headings like \"Required:\" or \"Recommended:\"; skip single-word headings\n",
    "        if re.match(r'^[A-Za-z- ]+:\\s*$', ml_strip) and ':' not in ml_strip.rstrip(':'):\n",
    "            # if it's only a heading like \"Required:\" keep it as a heading entry with empty value\n",
    "            # but more commonly these are just separators; we skip them to avoid noise\n",
    "            # skip if the line ends with \":\" and nothing after\n",
    "            if re.match(r'^[A-Za-z -]+:\\s*$', ml_strip):\n",
    "                continue\n",
    "        m = re.match(r'^\\s*([^:]+)\\s*:\\s*(.*)$', ml_strip)\n",
    "        if m:\n",
    "            key = m.group(1).strip()\n",
    "            value = m.group(2).strip()\n",
    "            metadata[key] = value\n",
    "        else:\n",
    "            # fallback: store whole line with auto key\n",
    "            metadata_key = f\"_meta_line_{len(metadata)+1}\"\n",
    "            metadata[metadata_key] = ml_strip\n",
    "\n",
    "    csv_text = \"\\n\".join(lines[csv_start_index:]).lstrip(\"\\n\")\n",
    "    return metadata, csv_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ac0657-e1bb-4eba-9f00-58c4ccadd3bf",
   "metadata": {},
   "source": [
    "### Checking for Metadata Completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2ebc627-cdec-45cc-a0e0-b992e3e3839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Metadata validation ----------\n",
    "def check_metadata_completeness(metadata: Dict[str, str], required_keys: List[str]) -> Dict[str, Any]:\n",
    "    problems = []\n",
    "    for k in required_keys:\n",
    "        if k not in metadata:\n",
    "            problems.append(f\"Missing required metadata key: '{k}'\")\n",
    "        else:\n",
    "            if metadata[k] is None or str(metadata[k]).strip() == \"\":\n",
    "                problems.append(f\"Metadata key '{k}' is present but empty\")\n",
    "\n",
    "    if \"field_delimiter\" in metadata:\n",
    "        if metadata[\"field_delimiter\"] not in [\",\", \";\", \"\\t\", \"|\", \":\", \" \"]:\n",
    "            problems.append(\n",
    "                f\"Unusual field_delimiter '{metadata['field_delimiter']}'. If your CSV uses commas, set field_delimiter to ','\"\n",
    "            )\n",
    "    return {\"ok\": len(problems) == 0, \"problems\": problems}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f55f5c6-0012-403d-bc74-6a49c82fce1f",
   "metadata": {},
   "source": [
    "### Building a Frictionless Schema from the Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8836258c-3cf0-4428-a69a-60252daae27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Build frictionless schema from metadata OR fallback descriptor ----------\n",
    "def build_frictionless_schema_from_metadata(metadata: Dict[str, str],\n",
    "                                            fallback_descriptor: Optional[Dict] = None,\n",
    "                                            df: Optional[pd.DataFrame] = None) -> Schema:\n",
    "    if \"fields\" in metadata:\n",
    "        val = metadata[\"fields\"]\n",
    "        fields = None\n",
    "        try:\n",
    "            parsed = json.loads(val)\n",
    "            if isinstance(parsed, list):\n",
    "                fields = parsed\n",
    "        except Exception:\n",
    "            fields = [c.strip() for c in re.split(r'[,;|:]+', val) if c.strip()]\n",
    "        if fields and df is not None:\n",
    "            descriptor_fields = []\n",
    "            for col in fields:\n",
    "                if col not in df.columns:\n",
    "                    descriptor_fields.append({\"name\": col, \"type\": \"string\", \"constraints\": {\"required\": False}})\n",
    "                else:\n",
    "                    dtype = df[col].dtype\n",
    "                    if pd.api.types.is_integer_dtype(dtype):\n",
    "                        ftype = \"integer\"\n",
    "                    elif pd.api.types.is_float_dtype(dtype):\n",
    "                        ftype = \"number\"\n",
    "                    else:\n",
    "                        ftype = \"string\"\n",
    "                    descriptor_fields.append({\"name\": col, \"type\": ftype})\n",
    "            descriptor = {\"fields\": descriptor_fields}\n",
    "            return Schema(descriptor)\n",
    "\n",
    "    if \"schema\" in metadata:\n",
    "        try:\n",
    "            descriptor = json.loads(metadata[\"schema\"])\n",
    "            return Schema(descriptor)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if fallback_descriptor is not None:\n",
    "        return Schema(fallback_descriptor)\n",
    "\n",
    "    if df is not None:\n",
    "        descriptor_fields = []\n",
    "        for col in df.columns:\n",
    "            dtype = df[col].dtype\n",
    "            if pd.api.types.is_integer_dtype(dtype):\n",
    "                ftype = \"integer\"\n",
    "            elif pd.api.types.is_float_dtype(dtype):\n",
    "                ftype = \"number\"\n",
    "            else:\n",
    "                unique_vals = df[col].dropna().unique()\n",
    "                if 0 < len(unique_vals) <= 10 and all(isinstance(v, str) for v in unique_vals):\n",
    "                    ftype = \"string\"\n",
    "                    descriptor_fields.append({\"name\": col, \"type\": ftype, \"constraints\": {\"required\": False, \"enum\": list(map(str, unique_vals))}})\n",
    "                    continue\n",
    "                ftype = \"string\"\n",
    "            descriptor_fields.append({\"name\": col, \"type\": ftype, \"constraints\": {\"required\": False}})\n",
    "        descriptor = {\"fields\": descriptor_fields, \"missingValues\": [\"\", \"NA\"]}\n",
    "        return Schema(descriptor)\n",
    "\n",
    "    raise ValueError(\"Cannot build schema: no metadata, no fallback, no dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777a91e9-de7b-4c10-9912-b28f67cd6656",
   "metadata": {},
   "source": [
    "### Validating Data with Metadata Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86089cfd-1bc9-42b9-9752-03baa0d49d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Validate data with frictionless and collect readable errors ----------\n",
    "def validate_data_with_schema(df: pd.DataFrame, schema: Schema, csv_path: Optional[Path] = None) -> Dict[str, Any]:\n",
    "    if csv_path is not None:\n",
    "        resource = Resource(path=str(csv_path), schema=schema)\n",
    "    else:\n",
    "        resource = Resource(data=df, schema=schema)\n",
    "\n",
    "    report = validate(resource)\n",
    "    errors = []\n",
    "    try:\n",
    "        flattened = report.flatten() if hasattr(report, \"flatten\") else None\n",
    "        if flattened:\n",
    "            for e in flattened:\n",
    "                errors.append(e)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if not errors:\n",
    "        if hasattr(report, \"errors\") and report.errors:\n",
    "            errors = report.errors\n",
    "        else:\n",
    "            try:\n",
    "                tasks = report.to_descriptor().get(\"tasks\", [])\n",
    "                for t in tasks:\n",
    "                    for e in t.get(\"errors\", []):\n",
    "                        errors.append(e)\n",
    "            except Exception:\n",
    "                errors = [str(report)]\n",
    "\n",
    "    return {\"valid\": report.valid, \"errors\": errors, \"report\": report}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717a89f1-4f80-4e0d-b04d-1d906290ca16",
   "metadata": {},
   "source": [
    "### Generating Human Readable Error Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4a216ba-dc13-483f-974b-5637e5a356b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Human-readable error report generator ----------\n",
    "def human_readable_report(metadata_checks: Dict[str, Any],\n",
    "                          metadata: Dict[str, str],\n",
    "                          data_validation: Dict[str, Any]) -> str:\n",
    "    lines = []\n",
    "    lines.append(\"==== METADATA CHECK ====\")\n",
    "    if metadata_checks[\"ok\"]:\n",
    "        lines.append(\"Metadata status: OK (required keys are present).\")\n",
    "    else:\n",
    "        lines.append(\"Metadata status: PROBLEMS FOUND\")\n",
    "        for p in metadata_checks[\"problems\"]:\n",
    "            lines.append(f\"  - {p}\")\n",
    "\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Metadata content:\")\n",
    "    if metadata:\n",
    "        for k, v in metadata.items():\n",
    "            lines.append(f\"  {k}: {v}\")\n",
    "    else:\n",
    "        lines.append(\"  (No metadata detected)\")\n",
    "\n",
    "    lines.append(\"\\n==== SCHEMA USED (frictionless descriptor) ====\")\n",
    "    try:\n",
    "        schema_desc = data_validation[\"report\"].to_descriptor().get(\"tasks\", [{}])[0].get(\"resource\", {}).get(\"schema\")\n",
    "        if not schema_desc and \"report\" in data_validation:\n",
    "            schema_obj = data_validation[\"report\"]\n",
    "            schema_desc = getattr(schema_obj, \"schema\", None)\n",
    "    except Exception:\n",
    "        schema_desc = None\n",
    "\n",
    "    if schema_desc:\n",
    "        lines.append(json.dumps(schema_desc, indent=2))\n",
    "    else:\n",
    "        lines.append(\" (failed to retrieve descriptor; schema object available in program output)\")\n",
    "\n",
    "    lines.append(\"\\n==== DATA VALIDATION ====\")\n",
    "    if data_validation[\"valid\"]:\n",
    "        lines.append(\"Data validation: PASSED\")\n",
    "    else:\n",
    "        lines.append(\"Data validation: FAILED\")\n",
    "        lines.append(\"Errors found:\")\n",
    "        errs = data_validation[\"errors\"]\n",
    "        if not errs:\n",
    "            lines.append(\"  (No structured errors available; see full frictionless report below.)\")\n",
    "            lines.append(str(data_validation.get(\"report\")))\n",
    "        else:\n",
    "            for e in errs:\n",
    "                if isinstance(e, dict):\n",
    "                    code = e.get(\"code\") or e.get(\"error\") or e.get(\"type\")\n",
    "                    msg = e.get(\"message\") or e.get(\"note\") or str(e)\n",
    "                    row = e.get(\"rowNumber\") or e.get(\"row\") or e.get(\"row-number\")\n",
    "                    field = e.get(\"fieldNumber\") or e.get(\"fieldName\") or e.get(\"name\")\n",
    "                    loc = []\n",
    "                    if row is not None:\n",
    "                        loc.append(f\"row {row}\")\n",
    "                    if field:\n",
    "                        loc.append(f\"field '{field}'\")\n",
    "                    loc_text = \", \".join(loc) if loc else \"unknown location\"\n",
    "                    lines.append(f\"  - [{code}] {msg}  ({loc_text})\")\n",
    "                else:\n",
    "                    lines.append(f\"  - {str(e)}\")\n",
    "\n",
    "    lines.append(\"\\n==== SUGGESTED FIXES ====\")\n",
    "    if not metadata_checks[\"ok\"]:\n",
    "        lines.append(\"Metadata suggestions:\")\n",
    "        for p in metadata_checks[\"problems\"]:\n",
    "            lines.append(f\"  - {p}\")\n",
    "        lines.append(\"  - Ensure metadata keys follow the icsv structural guidelines (e.g. 'field_delimiter', 'geometry', 'srid', 'fields', ...).\")\n",
    "    else:\n",
    "        lines.append(\"Metadata looks OK (see above).\")\n",
    "\n",
    "    if not data_validation[\"valid\"]:\n",
    "        lines.append(\"Data suggestions:\")\n",
    "        for e in data_validation[\"errors\"]:\n",
    "            if isinstance(e, dict):\n",
    "                msg = e.get(\"message\") or e.get(\"note\") or str(e)\n",
    "                field = e.get(\"fieldName\") or e.get(\"field\") or e.get(\"fieldNumber\")\n",
    "                row = e.get(\"rowNumber\") or e.get(\"row\")\n",
    "                if field or row:\n",
    "                    lines.append(f\"  - Fix {('field '+str(field)) if field else ''} {('in row '+str(row)) if row else ''}: {msg}\")\n",
    "                else:\n",
    "                    lines.append(f\"  - {msg}\")\n",
    "            else:\n",
    "                lines.append(f\"  - {str(e)}\")\n",
    "        lines.append(\"  - Common fixes: remove or correct non-numeric tokens in numeric columns (e.g. 'error', 'red'), replace empty required cells with valid values or 'NA' if allowed, remove entirely empty rows.\")\n",
    "    else:\n",
    "        lines.append(\"Data looks valid with respect to the schema.\")\n",
    "\n",
    "    lines.append(\"\\n==== END OF REPORT ====\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf967c0a-4900-4c79-975f-c71058e61310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Main routine tying everything together ----------\n",
    "def validate_icsv_file(path: str,\n",
    "                       required_metadata_keys: List[str] = REQUIRED_METADATA_KEYS,\n",
    "                       fallback_schema_descriptor: Optional[Dict] = FALLBACK_METADATA_SCHEMA_DESCRIPTOR) -> Dict[str, Any]:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "\n",
    "    metadata, csv_text = split_metadata_and_csv(p)\n",
    "\n",
    "    # Prefer delimiter from metadata if present\n",
    "    delimiter = metadata.get(\"field_delimiter\", None)\n",
    "    if delimiter:\n",
    "        if delimiter.lower() in [\":\", \"colon\"]:\n",
    "            delim = \":\"\n",
    "        elif delimiter.lower() in [\",\", \"comma\"]:\n",
    "            delim = \",\"\n",
    "        elif delimiter.lower() in [\";\", \"semicolon\"]:\n",
    "            delim = \";\"\n",
    "        elif delimiter.lower() in [\"\\t\", \"tab\"]:\n",
    "            delim = \"\\t\"\n",
    "        else:\n",
    "            delim = delimiter\n",
    "    else:\n",
    "        delim = None\n",
    "\n",
    "    # Read into DataFrame (try using the file path first so Frictionless can pick up native parsing)\n",
    "    try:\n",
    "        if delim:\n",
    "            df = pd.read_csv(p, delimiter=delim, na_values=[\"NA\", \"\"])\n",
    "        else:\n",
    "            df = pd.read_csv(p, na_values=[\"NA\", \"\"])\n",
    "    except Exception:\n",
    "        # fallback: read csv_text (the portion we sliced) - allow pandas to infer\n",
    "        try:\n",
    "            df = pd.read_csv(pd.compat.StringIO(csv_text), na_values=[\"NA\", \"\"])\n",
    "        except Exception:\n",
    "            # ultimate fallback: parse with csv.reader\n",
    "            rows = list(csv.reader(csv_text.splitlines()))\n",
    "            df = pd.DataFrame(rows[1:], columns=rows[0] if rows else None)\n",
    "\n",
    "    metadata_checks = check_metadata_completeness(metadata, required_metadata_keys)\n",
    "    schema = build_frictionless_schema_from_metadata(metadata, fallback_descriptor=fallback_schema_descriptor, df=df)\n",
    "    data_validation = validate_data_with_schema(df=df, schema=schema, csv_path=p)\n",
    "    report_text = human_readable_report(metadata_checks, metadata, data_validation)\n",
    "\n",
    "    return {\n",
    "        \"metadata\": metadata,\n",
    "        \"metadata_checks\": metadata_checks,\n",
    "        \"schema\": schema,\n",
    "        \"dataframe\": df,\n",
    "        \"data_validation\": data_validation,\n",
    "        \"report_text\": report_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6492edae-e51e-4578-8686-531a0e58fe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biomass_sample.csv not found in current directory.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Demo run on your biomass_sample.csv example ----------\n",
    "if __name__ == \"__main__\":\n",
    "    sample_csv_path = Path(\"biomass_sample.csv\")\n",
    "    if not sample_csv_path.exists():\n",
    "        print(\"biomass_sample.csv not found in current directory.\")\n",
    "    else:\n",
    "        result = validate_icsv_file(str(sample_csv_path))\n",
    "        print(result[\"report_text\"])\n",
    "        Path(\"biomass_sample_validation_report.txt\").write_text(result[\"report_text\"], encoding=\"utf-8\")\n",
    "        print(\"\\nFull report written to biomass_sample_validation_report.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07468a6d-c9a1-45c4-98b5-00c269ccbe29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
